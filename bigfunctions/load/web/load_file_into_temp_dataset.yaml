type: function_py
author: Antoine Giraud
hide_in_doc: true
description: |
  Download web file into a temp dataset
  in bigfunctions project.

  ![graph load file](./load_file.png)

  Each call to this function **creates a new temporary dataset** which:

  - will contain the `destination_table` with the file data.
  - is accessible only to you (who calls the function) and the function. You have permission to read data, delete the tables and delete the dataset.
  - has a limited period of life. Default expiration time is set to 1h so that every table created will be automatically deleted after 1h. Empty datasets are periodically removed.
  - has a random name

  **File Data is downloaded using [ibis](https://ibis-project.org/) with [DuckDB](https://duckdb.org/)**. Available `file_type` values are:

  - **csv** : *[doc](https://ibis-project.org/backends/duckdb#ibis.backends.duckdb.Backend.read_csv)*
  - **json** : *[doc](https://ibis-project.org/backends/duckdb#ibis.backends.duckdb.Backend.read_json)*
  - **parquet** : *[doc](https://ibis-project.org/backends/duckdb#ibis.backends.duckdb.Backend.read_parquet)*
  - **delta** : *[doc](https://ibis-project.org/backends/duckdb#ibis.backends.duckdb.Backend.read_delta)*
  - **geo** : *[doc](https://ibis-project.org/backends/duckdb#ibis.backends.duckdb.Backend.read_geo)*. (this uses GDAL under the hood and enable you to also read **.xls**, **.xlsx**, **.shp** ...)
arguments:
  - name: url
    type: string
  - name: file_type
    type: string
  - name: options
    type: string
output:
  name: destination_table
  type: string
examples:
  - description: "load random csv"
    arguments:
      - "https://raw.githubusercontent.com/AntoineGiraud/dbt_hypermarche/refs/heads/main/input/achats.csv"
      - "csv"
      - null
    output: "bigfunctions.temp_6bdb75ca_7f72_4f1f_b46a_6ca59f7f66ac.file_data"
  - description: "load json - french departements"
    arguments:
      - "https://geo.api.gouv.fr/departements?fields=nom,code,codeRegion,region"
      - "json"
      - null
    output: "bigfunctions.temp_6bdb75ca_7f72_4f1f_b46a_6ca59f7f66ac.file_data"
  - description: "load parquet on Google Cloud Storage"
    arguments:
      - "gs://bike-sharing-history/toulouse/jcdecaux/2024/Feb.parquet"
      - "parquet"
      - null
    output: "bigfunctions.temp_6bdb75ca_7f72_4f1f_b46a_6ca59f7f66ac.file_data"
  - description: "load xls or xlsx"
    arguments:
      - "https://github.com/AntoineGiraud/dbt_hypermarche/raw/refs/heads/main/input/Hypermarche.xlsx"
      - "geo"
      - '{"layer":"Retours", "open_options": ["HEADERS=FORCE"]}'
    output: "bigfunctions.temp_6bdb75ca_7f72_4f1f_b46a_6ca59f7f66ac.file_data"
  - description: "load french tricky csv"
    arguments:
      - "https://www.data.gouv.fr/fr/datasets/r/323af5b8-7831-445b-9a46-d4da140b61b6"
      - "csv"
      - |

          '''{
            "columns": {
                "code_commune_insee": "VARCHAR",
                "nom_commune_insee": "VARCHAR",
                "code_postal": "VARCHAR",
                "lb_acheminement": "VARCHAR",
                "ligne_5": "VARCHAR"
            },
            "delim": ";",
            "skip": 1
          }'''
    output: "bigfunctions.temp_6bdb75ca_7f72_4f1f_b46a_6ca59f7f66ac.file_data"
init_code: | #python
  import tempfile
  import urllib.request
  import json

  import ibis
  import google.cloud.bigquery
  import google.api_core.exceptions
  import slugify.slugify


  def download_file(url, file_type, options):
    with tempfile.TemporaryDirectory() as folder:

      # DuckDB st_read needs local file
      if file_type in ('geo'):
        urllib.request.urlretrieve(url, f'{folder}/geo_file')
        url = f'{folder}/geo_file'

      # let's fetch the file with ibis & DuckDB
      con = ibis.duckdb.connect()
      read = {
        "csv": con.read_csv,
        "json": con.read_json,
        "parquet": con.read_parquet,
        "geo": con.read_geo,
        "delta": con.read_delta
      }[file_type]
      return read(url, **options).to_pandas()


  def upload_dataframe(df, destination_table, url):
    job_config = google.cloud.bigquery.LoadJobConfig(
      write_disposition="WRITE_TRUNCATE",
      destination_table_description=f'File downloaded by BigFunctions from the web url: {url}'
    )
    try:
      bigquery.load_table_from_dataframe(df, destination_table, job_config=job_config).result()
    except (google.api_core.exceptions.Forbidden, google.api_core.exceptions.NotFound, google.api_core.exceptions.PermissionDenied) as e:
      assert False, f'Service Account `{get_current_service_account()}` does not have data-editor permission for given destination dataset (or the dataset does not exsit). Please add it'
code: | #python
  assert file_type in ("csv", "json", "parquet", "geo", "delta"), "invalid type: accepted values: csv, json, parquet, geo, delta"
  assert url, "invalid url: it is null or empty"
  assert url.startswith( ("http://", "https://", "hf://", "s3://", "az://", "abfss://", "gs://") ), "invalid url: must start with: 'http://', 'https://', 'hf://', 's3://', 'az://', 'abfss://', 'gs://'"

  try:
    options = json.loads(options) if options else {}
  except Exception as e:
    assert False, "options argument is an invalid json " + repr(e)

  destination_dataset = bigquery.create_temp_dataset()
  destination_table = f'{destination_dataset}.file_data'

  df = download_file(url, file_type, options)
  df.columns = [slugify.slugify(col, separator="_") for col in df.columns]
  upload_dataframe(df, destination_table, url)
  return destination_table
requirements: |
  ibis-framework[duckdb]
  google-cloud-bigquery
  geopandas
  python-slugify
quotas:
  max_rows_per_query: 10
cloud_run:
  memory: 2048Mi
  concurrency: 1
  service_account: load-file@bigfunctions.iam.gserviceaccount.com
