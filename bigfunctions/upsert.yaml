type: procedure
category: utils
author:
  name: Axel Thevenot
  url: https://www.linkedin.com/in/axel-thevenot/
  avatar_url: "https://avatars.githubusercontent.com/u/39374103?v=4"
description: |-
  Merges `query_or_table_or_view` into the `destination_table`.

  A record is identified by its `primary_keys`. A unique combination of those fields is a unique record.
  Before the merging operation, the records are identified and deduplicated according to the `primary_keys`.
  If `recency_field` is filled then the last record version is kept else it is chosen arbitrarily.


  | Param  | Possible values  |
  |---|---|
  | `query_or_table_or_view` | Can be a fully qualified table or view `(<project-id>.)?<dataset_id>.<table_or_view_name>`. <br> Can also be a plain query in BigQuery Standard SQL. |
  | `destination_table` | Must be a fully qualified table `(<project-id>.)?<dataset_id>.<table_or_view_name>`. |
  | `insertion_mode` | Three insertion mode are available:<ul><li> `"insert_only"`: existing records in `query_or_table_or_view` and not existing in `destination_table` are inserted. Deletion and update are not possible. </li><li> `"delta"`: same as `insert_only` with the updatable records. Records existing both in `query_or_table_or_view` and in  `destination_table` are updated. If `recency_field` is filled, only the most recent version from source and destination is kept. </li><li> `"full"`: same as `delta` with the deletable records. Records not existing in `query_or_table_or_view` and existing in `destination_table` are deleted. </li> </ul> |
  | `primary_keys` | Combination of field identifying a record. If `primary_keys = []`, every row will be considered as a unique record. |
  | `recency_field` | Orderable field (ie. `timestamp`, `integer`, ...) to identify the relative freshness of a record version. |
arguments:
  - name: query_or_table_or_view
    type: string
  - name: destination_table
    type: string
  - name: insertion_mode
    type: string
  - name: primary_keys
    type: array<string>
  - name: recency_field
    type: string
examples:
  - description: "Merge tables in delta mode"
    arguments:
      - "'dataset_id.source_table_or_view'"
      - "'dataset_id.destination_table'"
      - "'delta'"
      - "['id']"
      - "'timestamp_field'"
    region: ALL
  - description: "Merge from query in full"
    arguments:
      - "'select * from dataset_id.source_table_or_view where filter_field = true'"
      - "'dataset_id.destination_table'"
      - "'full'"
      - "['id']"
      - "null"
    region: ALL
code: |
  declare query string;
  declare context json;
  declare table_columns array<string>;

  assert lower(insertion_mode) in ('insert_only', 'delta', 'full') AS '`insertion_mode` must be either "insert_only", "delta", or "full"';

  /*
    Get destination table columns to define the insert and update parts of the merge query.
    All the fields are returned excecpt hidden ones (example: _PARITIONTIME)
  */
  set query = '''
    {%- set split_destination_table = destination_table.split('.') %}

    {%- if split_destination_table | length == 3 %}
      {%- set project_dataset = split_destination_table[0] + "." + split_destination_table[1] %}
      {%- set table_name = split_destination_table[2] %}
    {%- else %}
      {%- set project_dataset = split_destination_table[0] %}
      {%- set table_name = split_destination_table[1] %}
    {%- endif -%}

    with table_columns as (
      select
        column_name
      from {{ project_dataset }}.INFORMATION_SCHEMA.COLUMNS
      where true
        and table_name = "{{ table_name }}"
        and is_hidden = "NO"
      order by
        ordinal_position
    )
    select
      array_agg(column_name)
    from table_columns
  '''
  ;

  set context = to_json(struct(
    destination_table as destination_table
  ));

  execute immediate {BIGFUNCTIONS_DATASET}.render_string(query, to_json_string(context)) into table_columns;

  /*
    Perform the merge query.
  */
  set query = '''
    merge into {{ destination_table }} T
    using (
      with query as (
        {{ query_or_table_or_view }}
      )
      select
        *,
      from query
      /*
        Deduplicate rows according to the primary keys.
        If the recency field is given then chose the most recent record from duplicated. Else chose arbitrarily.
        If no primary keys, then deduplicates the rows as it was a SELECT DISTINCT statemenet.
      */
      {%- if primary_keys | length >= 1 %}
      qualify 1 = row_number() over(
        partition by
        {%- for primary_key in primary_keys %}
          {{ primary_key }} {{- "," if not loop.last else "" }}
        {%- endfor %}
        {%- if recency_field is not none %}
        order by
          {{ recency_field }} desc
        {%- endif %}
      )
      {%- endif %}
    ) as S
    on true
    {%- for primary_key in primary_keys %}
      and T.{{ primary_key }} = S.{{ primary_key }}
    {%- endfor %}
    when not matched by target then
    insert (
      {%- for column_name in table_columns %}
      {{ column_name }} {{- "," if not loop.last else "" }}
      {%- endfor %}
    ) values (
      {%- for column_name in table_columns %}
      S.{{ column_name }} {{- "," if not loop.last else "" }}
      {%- endfor %}
    )
    {%- if insertion_mode  == 'full' %}
    when not matched by source then
      delete
    {%- endif %}
    {%- if insertion_mode  != 'insert_only' %}
    when matched
      /*
        Check for most recent record between source and destination.
        If no recency field, the source record is considered as the most up to date record.
      */
      {%- if recency_field is not none %} and coalesce(S.{{ recency_field }} >= T.{{ recency_field }}, true) {%- endif %}
    then
      update set
      {%- for column_name in table_columns %}
        T.{{ column_name }} = S.{{ column_name }} {{- "," if not loop.last else "" }}
      {%- endfor %}
    {%- endif %}
  '''
  ;

  set context = to_json(struct(
    if(
      regexp_contains(replace(trim(query_or_table_or_view), '`', ''), r'^(([a-zA-Z0-9\-]+)\.)?([a-zA-Z0-9_]+)\.([a-zA-Z0-9_]+)$'),
      'select * from ' || query_or_table_or_view,
      query_or_table_or_view
    )                 as query_or_table_or_view,
    destination_table as destination_table,
    insertion_mode    as insertion_mode,
    primary_keys      as primary_keys,
    recency_field     as recency_field,
    table_columns     as table_columns
  ));

  execute immediate {BIGFUNCTIONS_DATASET}.render_string(query, to_json_string(context));
