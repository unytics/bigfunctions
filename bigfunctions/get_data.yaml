type: function_py
category: get_data
author:
  name: Paul Marcombes
  url: https://www.linkedin.com/in/paul-marcombes
  avatar_url: "https://lh3.googleusercontent.com/a-/ACB-R5RDf2yxcw1p_IYLCKmiUIScreatDdhG8B83om6Ohw=s260"
description: |
  Get data from 250+ sources using [Airbyte Python Connectors](https://docs.airbyte.com/using-airbyte/pyairbyte/getting-started#available-connectors)
  .

  - [Airbye Serverless](https://github.com/unytics/airbyte_serverless) will extract data from `source` (one of 250+ Airbyte Python Connectors available on [PyPI](https://pypi.org/search/?q=airbyte-source-)) using `source_config` (source configuration in yaml format expected by Airbyte Serverless).
  - It will append data in raw format into one table per stream (a stream is like a resource type) in `destination_dataset`.
  - When supported by the stream, data is extracted incrementally (next execution will only retrieve new rows). For this purpose, a state is stored in `_airbyte_states` table in `destination_dataset`.
  - While running, connector logs are appended in table `_airbyte_logs`. You can check them to get details on the run.
  - Examples below explain how to set the arguments.
arguments:
  - name: source
    type: string
  - name: source_config
    type: string
  - name: streams
    type: string
  - name: destination_dataset
    type: string
output:
  name: result
  type: string
examples:
  - description: |
      Show sources available for `source` argument by setting `source` to `null`

      You can then copy one of these sources for `source` argument.
    arguments:
      - "null"
      - "null"
      - "null"
      - "null"
    output: |
      # AVAILABLE SOURCES

      airbyte-source-activecampaign==0.1.10
      airbyte-source-adjust==0.1.11
      airbyte-source-aha==0.3.10
      ...
  - description: |
      Show `source_config` sample at expected format by setting `source_config` to `null`.

      You can then copy the result, modify it and provide it as `source_config` argument.
    arguments:
      - "'airbyte-source-spacex-api==0.1.11'"
      - "null"
      - "null"
      - "null"
    output: |
      # SOURCE CONFIG

      id: # OPTIONAL | string
      options: # OPTIONAL | string
  - description: |
      **Encrypt Secrets!**

      You usually have to write secrets in `source_config` such as api keys.

      **We strongly advise NOT TO write your secrets in `source_config` in plain text**.

      Otherwise, they will be stored in plain text in your BigQuery logs for months.

      Instead, you can use the following snippet to generate an encrypted version of your secrets
      that you can safely copy in `source_config` as shown in example below.
      This public bigfunction (deployed on bigfunctions GCP project) will be able to decrypt them.
      But no one else can.

      <div>
        <input id="secret-to-encrypt" type="text" class="md-input" placeholder="a secret value">
        <button class="md-button md-button--primary" onclick="encrypt();">Encrypt Secret</button>
      </div>

      <script src="https://cdn.jsdelivr.net/npm/node-forge@1.0.0/dist/forge.min.js"></script>
      <script>
      const pem = `
      -----BEGIN PUBLIC KEY-----
      MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAsgotixwX/WsbKxeT3sef
      z6acpTomZEYOBJzIRLtcHyxzyMEuMN9iYF32l4t6K70b+38mLIO7K6LkZj/nnhrF
      UuPfAKme5R2Y4FqtJVSzplCmZc2/274eanIGi6BZ+kHOoc+eJTOEi3Wzp/EQ77iG
      bC9tD/0ZkXYdXzPHcJT0dOFHIV6wpLr2i251NAIDTI4CV1afqhxU313f5Wrw+pYa
      eUh2/v6KtvmLRQb3P4EUytQF7pBYx8kRGMLvmFja5Gg8Xj3XhXXsdjUkmAYumRDl
      HkUeB62F+TO1oOfTrwBAKRh8WfHJOyfDJyNw1EndZ4bz28ODxth4r+phaaSE+53y
      WQIDAQAB
      -----END PUBLIC KEY-----
      `;

      const publicKey = forge.pki.publicKeyFromPem(pem);


      function encrypt() {
        const plainText = document.getElementById('secret-to-encrypt').value;
        const encrypted = publicKey.encrypt(plainText, 'RSA-OAEP', {
          md: forge.md.sha256.create(),
          mgf1: { md: forge.md.sha256.create() }
        });
        const encryptedB64 = forge.util.encode64(encrypted);
        navigator.clipboard.writeText(`ENCRYPTED_SECRET(${encryptedB64})`);
        alert("Successfully copied the encrypted secret in clipboard. Paste it in your source_config as shown in example.");
      }
      </script>

    arguments:
      - "'airbyte-source-zendesk-support==2.6.10'"
      - |
        '''
        credentials:
          access_token: ENCRYPTED_SECRET(kdoekdswlxzapdldpzlfpfd)
        '''
      - "null"
      - "null"
    output: "..."
  - description: |
      Show available streams by setting `streams` argument to `null`.

      You can then copy one or several of these streams (separate them with commas) for `streams` argument.
    arguments:
      - "'airbyte-source-spacex-api==0.1.11'"
      - |
        '''
        id: ""
        options: ""
        '''
      - "null"
      - "null"
    output: |
      # AVAILABLE STREAMS

      launches, capsules, company, crew, cores, dragons, landpads, payloads, history, rockets, roadster, ships, starlink
  - description: |
      Show one record of a stream by setting `destination_dataset` argument to `null`.

      This is useful to check that you can get a record successfully or to understand its content.
    arguments:
      - "'airbyte-source-spacex-api==0.1.11'"
      - |
        '''
        id: ""
        options: ""
        '''
      - "'launches'"
      - "null"
    output: |
      # FIRST RECORD

      {
          "stream": "launches",
          "data": {
              "rocket": "5e9d0d95eda69973a809d1ec",
              "success": true,
              "flight_number": 187,
              "name": "Crew-5",
              "date_utc": "2022-10-05T16:00:00.000Z",
              ...
          },
          "emitted_at": 1721804564098
      }
  - description: |
      Extract and load `crew` and `rockets` streams into `your_project.your_dataset`.

      > **Requirements**
      >
      > You must create the `destination_dataset` and give `dataEditor` access to `bigfunction@bigfunctions.iam.gserviceaccount.com` before calling this function.
      > You can do this by executing:
      >
      > ```sql
      > -- Create Destination Dataset
      > create schema `your_project.your_dataset`;
      >
      > -- Grant Access to Destination Dataset
      > grant `roles/bigquery.dataEditor`
      > on schema `your_project.your_dataset`
      > to 'serviceAccount:bigfunction@bigfunctions.iam.gserviceaccount.com';
      > ```
    arguments:
      - "'airbyte-source-spacex-api==0.1.11'"
      - |
        '''
        id: ""
        options: ""
        '''
      - "'crew, rockets'"
      - "'your_project.your_dataset'"
    output: "Data load job ended successfully. Check logs at `your_project.your_dataset._airbyte_logs`"
init_code: |
  import json
  import requests
  import yaml
  from airbyte_serverless.connections import Connection
  from airbyte_serverless.sources import ExecutableAirbyteSource
  from airbyte_serverless import airbyte_utils
  import google.api_core.exceptions


  SOURCES_URL = 'https://connectors.airbyte.com/files/registries/v0/oss_registry.json'


  def get_sources():
    resp = requests.get(SOURCES_URL)
    res = resp.json()
    sources = res['sources']
    python_sources = [
      f"{source.get('remoteRegistries', {}).get('pypi', {}).get('packageName')}=={source['dockerImageTag']}"
      for source in sources
      if source.get('remoteRegistries', {}).get('pypi', {}).get('enabled') is True
    ]
    return python_sources


  def get_yaml_definition_example(executable):
    airbyte_source =  ExecutableAirbyteSource(executable)
    spec = airbyte_source.spec
    yaml_config = airbyte_utils.generate_connection_yaml_config_sample(spec)
    if yaml_config.startswith('#'):
      yaml_config = '\n'.join(yaml_config.split('\n')[1:])
    return yaml_config


  def yaml2dict(source_yaml_config):
    try:
      return yaml.safe_load(source_yaml_config) or {}
    except:
      assert False, 'Given `source_config` is NOT a valid yaml content'


  def list_streams(executable, source_config):
    airbyte_source = ExecutableAirbyteSource(executable=executable, config=source_config)
    return airbyte_source.available_streams


  def get_first_record(executable, source_config, streams):
    airbyte_source = ExecutableAirbyteSource(executable=executable, config=source_config, streams=streams)
    return json.dumps(airbyte_source.first_record, indent=4)


  def run(executable, source_config, streams, destination_dataset):
    config = {
      'source': {
        'executable': executable,
        'config': source_config,
        'streams': streams,
      },
      'destination': {
        'connector': 'bigquery',
        'config': {
          'dataset': destination_dataset,
          'buffer_size_max': 1000,
        },
      },
    }
    yaml_config = yaml.dump(config)
    connection = Connection(yaml_config)
    try:
      connection.run()
    except (google.api_core.exceptions.Forbidden, google.api_core.exceptions.NotFound, google.api_core.exceptions.PermissionDenied) as e:
      assert False, f'Service Account `{get_current_service_account()}` does not have data-editor permission for given destination dataset (or the dataset does not exsit). Please add it'


  AVAILABLE_SOURCES = get_sources()

code: |
  source = source or ''
  source_is_unavailable = not source or not [s for s in AVAILABLE_SOURCES if s.split('==')[0] == source.split('==')[0]]
  if source_is_unavailable:
    return '# AVAILABLE SOURCES\n\n' + '\n'.join(AVAILABLE_SOURCES)

  executable = f'pipx run {source}'
  source_config = source_config or ''
  if not source_config.strip():
    source_config = get_yaml_definition_example(executable)
    return '# SOURCE CONFIG\n\n' + source_config

  source_config = yaml2dict(source_config)

  if not streams:
    streams = list_streams(executable, source_config)
    return '# AVAILABLE STREAMS\n\n' + ', '.join(streams)

  if not destination_dataset:
    return f'# FIRST RECORD\n\n' + get_first_record(executable, source_config, streams)

  run(executable, source_config, streams, destination_dataset)
  return f'Data load job ended successfully. Check logs at `{destination_dataset}._airbyte_logs`'
dockerfile:
  image: ubuntu:22.04
  apt_packages: python3.10-venv python3-pip
requirements: |
  airbyte-serverless
  cryptography
max_batching_rows: 1
quotas:
  max_rows_per_user_per_day: 200
cloud_run:
  memory: 512Mi
  max_instances: 10
  concurrency: 1
  timeout: 30m
